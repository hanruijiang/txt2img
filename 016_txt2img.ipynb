{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f137b32-8f60-4e77-936b-4e3a010bb088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from transformers import CLIPImageProcessor, AutoModel\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cc13ab-07ad-4053-9120-15b806b58d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-image-pipeline/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b654a425-c5c1-4bea-bc56-a20d87b8b8c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mINFO: Created a temporary directory at /tmp/tmpownu7pg5\u001b[0m\n",
      "\u001b[1;32mINFO: Writing /tmp/tmpownu7pg5/_remote_module_non_scriptable.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from stable_diffusion import StableDiffusion, CLIPTextEmbedder\n",
    "from stable_diffusion.utils_image import get_image_data\n",
    "from worker.image_generation.scripts.stable_diffusion_base_script import StableDiffusionBaseScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1afc4657-4b88-4086-80fe-b7ac5c0c7355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../kcg-ml-image-pipeline/input/model/sd/v1-5-pruned-emaonly/v1-5-pruned-emaonly.safetensors'\n",
    "tokenizer_path = '../kcg-ml-image-pipeline/input/model/clip/txt_emb_tokenizer'\n",
    "transformer_path = '../kcg-ml-image-pipeline/input/model/clip/txt_emb_model'\n",
    "\n",
    "# prompt_path = './generated/prompt/empty.tsv'\n",
    "prompt_path = './generated/temperature/prompt/2023-11-30-independent-approx-v1-08-environmental.csv'\n",
    "OUTPUT_DIR = './generated/temperature/8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59719a11-0793-42ed-a2cd-44aa980b56b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = \"ddim\"\n",
    "sampler_steps = 20\n",
    "cfg_strength=12\n",
    "image_width=512\n",
    "image_height=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f210aa-2a2f-4b1f-a7d0-d179f9891424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_DIR, 'image'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'meta'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'clip'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'embedding'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1c91f-0fe9-483f-9067-2130f58f7ca7",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e17c68-3fdc-4deb-a14a-02fd1030db3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt2img = StableDiffusionBaseScript(\n",
    "    sampler_name=sampler,\n",
    "    n_steps=sampler_steps,\n",
    "    force_cpu=False,\n",
    "    cuda_device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa7c81-bb0b-44ff-bd8a-46db917d7663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  encoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t1,611.43ms\u001b[0m                                      \n",
      "  decoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t530.85ms\u001b[0m                                        \n",
      "Autoencoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t2,150.81ms\u001b[0m                                    \n"
     ]
    }
   ],
   "source": [
    "txt2img.initialize_latent_diffusion(\n",
    "    autoencoder=None, \n",
    "    clip_text_embedder=None, \n",
    "    unet_model=None,\n",
    "    path=model_path, \n",
    "    force_submodels_init=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a05696-fdc8-45df-96e5-6df19ce05b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_text_embedder = CLIPTextEmbedder(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0671d5-e4f4-41e2-ad8b-11ec8cda2cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_text_embedder.load_submodels(\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    transformer_path=transformer_path\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c147c-b2c4-40cf-9bf4-96d0e0d70182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(positive_prompt, negative_prompt, seed=-1, cfg_strength=12, image_width=512, image_height=512):\n",
    "    \n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 2 ** 24 - 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        embedded_prompts, positive_pooler_output, _ = clip_text_embedder.forward_return_all(positive_prompt)\n",
    "        negative_embedded_prompts, negative_pooler_output, _ = clip_text_embedder.forward_return_all(negative_prompt)\n",
    "        \n",
    "        positive_pooler_output = positive_pooler_output.detach().cpu().numpy()\n",
    "        negative_pooler_output = negative_pooler_output.detach().cpu().numpy()\n",
    "\n",
    "        latent = txt2img.generate_images_latent_from_embeddings(\n",
    "            batch_size=1,\n",
    "            embedded_prompt=embedded_prompts,\n",
    "            null_prompt=negative_embedded_prompts,\n",
    "            uncond_scale=cfg_strength,\n",
    "            seed=seed,\n",
    "            w=image_width,\n",
    "            h=image_height\n",
    "        )\n",
    "\n",
    "        images = txt2img.get_image_from_latent(latent)\n",
    "\n",
    "        output_file_hash, img_byte_arr = get_image_data(images)\n",
    "    \n",
    "    return output_file_hash, img_byte_arr, seed, positive_pooler_output, negative_pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b995818-6e51-42b1-a15d-6eb652197ec2",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c085f0-1ee3-474e-b101-095209fe3bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompts = pd.read_csv(prompt_path, sep='\\t')\n",
    "prompts = pd.read_csv(prompt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da73413-5b3c-4055-8a85-3f803c571045",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c054579-d1a5-4c5d-8948-a8c75b755d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = set()\n",
    "for fname in os.listdir(os.path.join(OUTPUT_DIR, 'image')):\n",
    "    try:\n",
    "        Image.open(os.path.join(OUTPUT_DIR, 'image', fname))\n",
    "        np.load(os.path.join(OUTPUT_DIR, 'embedding', fname.replace('.jpg', '.npz')))\n",
    "        js = json.load(open(os.path.join(OUTPUT_DIR, 'meta', fname.replace('.jpg', '.json'))))\n",
    "        done.add((js['positive_prompt'], js['negative_prompt']))\n",
    "    except:\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, 'image', fname)):\n",
    "            os.system(f'rm {os.path.join(OUTPUT_DIR, \"image\", fname)}')\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, 'meta', fname.replace(\".jpg\", \".json\"))):\n",
    "            os.system(f'rm {os.path.join(OUTPUT_DIR, \"meta\", fname.replace(\".jpg\", \".json\"))}')\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, 'embedding', fname.replace('.jpg', '.npz'))):\n",
    "            os.system(f'rm {os.path.join(OUTPUT_DIR, \"embedding\", fname.replace(\".jpg\", \".npz\"))}')\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, 'clip', fname.replace('.jpg', '.npy'))):\n",
    "            os.system(f'rm {os.path.join(OUTPUT_DIR, \"clip\", fname.replace(\".jpg\", \".npy\"))}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292b2d7-5bc8-455d-b986-fee9d604cb5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d43033372384965a846e4c0219cb71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for positive_prompt, negative_prompt in tqdm(prompts[['positive_prompt', 'negative_prompt']].itertuples(index=False), total=prompts.shape[0]):\n",
    "\n",
    "    if (positive_prompt, negative_prompt) in done:\n",
    "        continue\n",
    "        \n",
    "    output_file_hash, img_byte_arr, seed, positive_pooler_output, negative_pooler_output = worker(positive_prompt, negative_prompt, seed=-1)\n",
    "    \n",
    "    creation_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    meta = dict(\n",
    "        positive_prompt=positive_prompt, \n",
    "        negative_prompt=negative_prompt,\n",
    "        file_hash=output_file_hash,\n",
    "        sampler=sampler,\n",
    "        sampler_steps=sampler_steps,\n",
    "        cfg_strength=cfg_strength,\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        creation_time=creation_time\n",
    "    )\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'image', f'{output_file_hash}.jpg'), 'wb') as f:\n",
    "        f.write(img_byte_arr.getbuffer())\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'meta', f'{output_file_hash}.json'), 'wt') as f:\n",
    "        json.dump(meta, f)\n",
    "        \n",
    "    np.savez(\n",
    "        os.path.join(OUTPUT_DIR, 'embedding', f'{output_file_hash}.npz'),\n",
    "        positive_pooler_output=positive_pooler_output,\n",
    "        negative_pooler_output=negative_pooler_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6507f9d-d14c-4d7e-89f8-75441216c0c6",
   "metadata": {},
   "source": [
    "# clip features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7da84a5f-d386-477e-925e-11f9869ef09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a273ab9-cb50-4fc3-b802-550e0e309f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = CLIPImageProcessor.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7eb2a3ce-29dd-497e-89cb-275d20ade96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = list()\n",
    "\n",
    "for file_name in os.listdir(os.path.join(OUTPUT_DIR, 'image')):\n",
    "    clip_path = os.path.join(OUTPUT_DIR, 'clip', file_name.replace('.jpg', '.npy'))\n",
    "    \n",
    "    if os.path.exists(clip_path):\n",
    "        continue\n",
    "        \n",
    "    file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1a62d58-e94c-45b6-872a-bc1c3a77d669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd726f0c39941788324faf953e00b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(file_names), BATCH_SIZE)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = list()\n",
    "        for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "            image = Image.open(os.path.join(OUTPUT_DIR, 'image', file_name))\n",
    "            image = preprocessor(images=image, return_tensors=\"pt\")\n",
    "            images.append(image['pixel_values'])\n",
    "\n",
    "        images = torch.concat(images, dim=0)\n",
    "    \n",
    "        image_features = clip_model.get_image_features(pixel_values=images.to(clip_model.device))\n",
    "        image_features = image_features.detach().cpu().numpy()\n",
    "        \n",
    "        for file_name, image_feature in zip(file_names[i:i+BATCH_SIZE], image_features):\n",
    "            clip_path = os.path.join(OUTPUT_DIR, 'clip', file_name.replace('.jpg', '.npy'))\n",
    "            np.save(clip_path, image_feature[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0900380-e106-45ec-a374-a4998fdd5627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
