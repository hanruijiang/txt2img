{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c10062-4aaf-43c5-9a56-0fcc54568679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import msgpack\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03dc12a2-26ae-408f-91cd-e701c69f7cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 77\n",
    "# MODEL_NAME = 'BAAI/bge-base-en-v1.5'\n",
    "MODEL_NAME = 'openai/clip-vit-large-patch14'\n",
    "# MODEL_NAME = 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'\n",
    "# MODEL_NAME = 'johngiorgi/declutr-base'\n",
    "# MAX_LENGTH = 88\n",
    "# MODEL_NAME = '../Llama-2-7b-hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1744b-72dc-427d-8256-a5e2499ea129",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ca2e84-26f7-4d4d-bbe7-6adc0df0f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = '../kcg-ml-image-pipeline/output/dataset/'\n",
    "\n",
    "DATASETs = [\n",
    "    # 'environmental', \n",
    "    # 'character', \n",
    "    # 'icons', \n",
    "    # 'mech', \n",
    "    'waifu',\n",
    "    'propaganda-poster'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb74eb-e538-42f7-9808-92a8cccfcb3a",
   "metadata": {},
   "source": [
    "## save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15b03202-afce-4df1-9c3d-456a7e2daaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(dataset_name):\n",
    "\n",
    "    paths = sorted(glob.glob(os.path.join(ROOT, 'data', dataset_name, '**/*_data.msgpack')))\n",
    "    \n",
    "    job_uuids = list()\n",
    "    file_paths = list()\n",
    "    file_hashs = list()\n",
    "    positive_prompts = list()\n",
    "    negative_prompts = list()\n",
    "    creation_times = list()\n",
    "    for path in tqdm(paths, leave=False):\n",
    "    \n",
    "        if not os.path.exists(path.replace('_data.msgpack', '_clip.msgpack').replace('/data/', '/clip/')):\n",
    "            continue\n",
    "    \n",
    "        with open(path, 'rb') as f:\n",
    "            mp = msgpack.load(f)\n",
    "        \n",
    "        job_uuids.append(mp['job_uuid'])\n",
    "        file_paths.append(mp['file_path'])\n",
    "        file_hashs.append(mp['file_hash'])\n",
    "        positive_prompts.append(mp['positive_prompt'])\n",
    "        negative_prompts.append(mp['negative_prompt'])\n",
    "        creation_times.append(mp['creation_time'])\n",
    "    \n",
    "    os.makedirs(os.path.join('data', dataset_name), exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        zip(file_paths, creation_times, job_uuids, positive_prompts, negative_prompts, file_hashs), \n",
    "        columns=['file_path', 'creation_time', 'job_uuid', 'positive_prompt', 'negative_prompt', 'file_hash']\n",
    "    )\n",
    "\n",
    "    df.drop_duplicates(['file_hash'], inplace=True)\n",
    "    df.set_index('file_hash', inplace=True)\n",
    "    \n",
    "    json.dump(\n",
    "        df.to_dict(orient='index'),\n",
    "        open(os.path.join('data', dataset_name, 'prompt.json'), 'w')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ba26c1-fa6d-4ff2-9606-e076021f2729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    save_json(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71358c40-3a87-451d-9d0b-bee09876493a",
   "metadata": {},
   "source": [
    "## load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b29d5f9-b972-44fa-8f84-a8f42773755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(dataset_name):\n",
    "\n",
    "    file_hashs = list()\n",
    "    file_paths = list()\n",
    "    positive_prompts = list()\n",
    "    negative_prompts = list()\n",
    "    \n",
    "    for file_hash, info in json.load(open(os.path.join('data', dataset_name, 'prompt.json'))).items():\n",
    "        \n",
    "        file_hashs.append(file_hash)\n",
    "        file_paths.append(info['file_path'])\n",
    "        positive_prompts.append(info['positive_prompt'])\n",
    "        negative_prompts.append(info['negative_prompt'])\n",
    "\n",
    "    return file_hashs, file_paths, positive_prompts, negative_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d19474-f270-4149-b7b8-3efe7989c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8634\n",
      "8816\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    file_hashs, file_paths, positive_prompts, negative_prompts = load_json(dataset_name)\n",
    "    print(len(file_hashs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5511b-ddfd-4dc9-92b9-ee9b78d8a193",
   "metadata": {},
   "source": [
    "# load text embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79377829-7d62-4df6-9f54-61f12ea33d78",
   "metadata": {},
   "source": [
    "## from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "019884b2-42d8-4c74-b41a-9f725b10c7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e2f96-d4c9-4ac0-bca0-f4f75cd98b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### from CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "559844df-0456-4fab-87ff-04f59a2749f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).text_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0490001b-a764-4abb-970e-a8c0d1ede92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts, use_penultimate=False):\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "    clip_text_opt = transformer(input_ids=tokens, output_hidden_states=True, return_dict=True)\n",
    "    \n",
    "    attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    \n",
    "    if use_penultimate:\n",
    "        last_hidden_state = clip_text_opt.hidden_states[-1].detach().cpu().numpy()\n",
    "    else:\n",
    "        last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580208c-b733-47bd-af51-9841bea563b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14127512-6084-4737-ac7d-4af8ae09517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def worker(texts):\n",
    "    \n",
    "#     batch_encoding = tokenizer(\n",
    "#         texts,\n",
    "#         truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "#         return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "#     clip_text_opt = transformer(input_ids=tokens)\n",
    "\n",
    "#     last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "#     attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    \n",
    "#     pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "#     # pooler_output = None\n",
    "    \n",
    "#     return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8a83a-eda4-46e2-8d0a-7e29194f3d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformer = AutoModel.from_pretrained(MODEL_NAME).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbadf41-a3c2-4bb2-aaa8-f0e8d5c99e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14a1ff-fb5b-42de-8143-447b1761a3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dccc6dafc5a4dcda5c7b144d0d42744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../Llama-2-7b-hf were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# transformer = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, load_in_8bit=True, device_map='auto').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360503-30e0-48d3-b0de-6f3d3ab4b63b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# embed & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52457393-ceb4-46fc-9ef3-c143cc7faf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_text_npz(dataset_name):\n",
    "\n",
    "    # load\n",
    "\n",
    "    file_hashs, file_paths, positive_prompts, negative_prompts = load_json(dataset_name)\n",
    "\n",
    "    # \n",
    "    \n",
    "    # positive_last_hidden_states = list()\n",
    "    # positive_attention_masks = list()\n",
    "    positive_pooler_outputs = list()\n",
    "    \n",
    "    # negative_last_hidden_states = list()\n",
    "    # negative_attention_masks = list()\n",
    "    negative_pooler_outputs = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in tqdm(range(0, len(positive_prompts), BATCH_SIZE), leave=False):\n",
    "            \n",
    "            last_hidden_state, pooler_output, attention_mask = worker(positive_prompts[i:i+BATCH_SIZE])\n",
    "            \n",
    "            # positive_last_hidden_states.append(last_hidden_state)\n",
    "            # positive_attention_masks.append(attention_mask)\n",
    "            if pooler_output is not None:\n",
    "                positive_pooler_outputs.append(pooler_output)\n",
    "        \n",
    "        for i in tqdm(range(0, len(negative_prompts), BATCH_SIZE), leave=False):\n",
    "            \n",
    "            last_hidden_state, pooler_output, attention_mask = worker(negative_prompts[i:i+BATCH_SIZE])\n",
    "            \n",
    "    #         negative_last_hidden_states.append(last_hidden_state)\n",
    "    #         negative_attention_masks.append(attention_mask)\n",
    "            if pooler_output is not None:\n",
    "                negative_pooler_outputs.append(pooler_output)\n",
    "    \n",
    "    # positive_last_hidden_states = np.concatenate(positive_last_hidden_states, axis=0)\n",
    "    # positive_attention_masks = np.concatenate(positive_attention_masks, axis=0)\n",
    "    if len(positive_pooler_outputs) > 0:\n",
    "        positive_pooler_outputs = np.concatenate(positive_pooler_outputs, axis=0)\n",
    "    \n",
    "    # negative_last_hidden_states = np.concatenate(negative_last_hidden_states, axis=0)\n",
    "    # negative_attention_masks = np.concatenate(negative_attention_masks, axis=0)\n",
    "    if len(positive_pooler_outputs) > 0:\n",
    "        negative_pooler_outputs = np.concatenate(negative_pooler_outputs, axis=0)\n",
    "\n",
    "    #\n",
    "    \n",
    "    np.savez(\n",
    "        os.path.join('data', dataset_name, 'clip_text_emb.npz'), \n",
    "        file_hashs=np.array(file_hashs), \n",
    "        file_paths=np.array(file_paths), \n",
    "        # positive_last_hidden_states=positive_last_hidden_states, \n",
    "        # positive_attention_masks=positive_attention_masks,\n",
    "        positive_pooler_outputs=positive_pooler_outputs,\n",
    "        # negative_last_hidden_states=negative_last_hidden_states,\n",
    "        # negative_attention_masks=negative_attention_masks,\n",
    "        negative_pooler_outputs=negative_pooler_outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c188adf-636d-48f5-b615-191ec1aa7e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    save_text_npz(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd5a93-65ff-4fb9-a8ff-b4f5200de3be",
   "metadata": {},
   "source": [
    "# vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e8360-f149-480d-8142-30e712685f3a",
   "metadata": {},
   "source": [
    "## from msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "739377e6-2e70-4200-bb69-101ef2fb56ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_vision_npz(dataset_name):\n",
    "    \n",
    "    # load\n",
    "\n",
    "    file_hashs, file_paths, positive_prompts, negative_prompts = load_json(dataset_name)\n",
    "\n",
    "    #\n",
    "    \n",
    "    vectors = list()\n",
    "    \n",
    "    for file_path in tqdm(file_paths, leave=False):\n",
    "        \n",
    "        msp_path = file_path.replace('_data.msgpack', '_clip.msgpack').replace('.jpg', '_clip.msgpack')\n",
    "    \n",
    "        with open(os.path.join(ROOT, 'clip', msp_path), 'rb') as f:\n",
    "            data = f.read()\n",
    "        decoded_data = msgpack.unpackb(data)\n",
    "        \n",
    "        vectors.append(np.array(decoded_data['clip-feature-vector']))\n",
    "\n",
    "    # save\n",
    "\n",
    "    np.savez(\n",
    "        os.path.join('data', dataset_name, 'clip_vision_emb.npz'), \n",
    "        file_hashs=np.array(file_hashs), \n",
    "        file_paths=np.array(file_paths), \n",
    "        image_embeds=np.concatenate(vectors, axis=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eed7dd3-a566-4ccc-8ff2-2d4be5fb07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    save_vision_npz(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150b12a-5d9e-4407-95f1-17db845f56b9",
   "metadata": {},
   "source": [
    "## from file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a233f80b-75af-4ae6-a153-bdc6206aeff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPUT_DIR = '../dataset/civitai-stable-diffusion-337k/images/'\n",
    "# OUTPUT_DIR = '../dataset/civitai-stable-diffusion-337k/clip/'\n",
    "\n",
    "INPUT_DIR = '../dataset/scrap/steam/screenshot/'\n",
    "OUTPUT_DIR = '../dataset/scrap/steam/clip/'\n",
    "\n",
    "# INPUT_DIR = '../dataset/scrap/leonardo/images/'\n",
    "# OUTPUT_DIR = '../dataset/scrap/leonardo/clip/'\n",
    "\n",
    "# INPUT_DIR = '../dataset/midjourney-messages/images/'\n",
    "# OUTPUT_DIR = '../dataset/midjourney-messages/clip/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75016e33-3f73-4f6a-81a6-2dbaa89f557d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ca42ef-3392-4963-a43d-73369118ca2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = CLIPImageProcessor.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced115f8-2ba4-48bb-bb24-70425cf6ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = list()\n",
    "\n",
    "for file_name in os.listdir(INPUT_DIR):\n",
    "    if not file_name.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "    clip_path = os.path.join(OUTPUT_DIR, f'{os.path.splitext(file_name)[0]}.npy')\n",
    "    \n",
    "    if os.path.exists(clip_path):\n",
    "        continue\n",
    "        \n",
    "    file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05d49ba-54bd-4172-9ee3-43573a08aa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9fd3cdc3674a4d88ab939fc4893492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(file_names), BATCH_SIZE)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = list()\n",
    "        names = list()\n",
    "        for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "            try:\n",
    "                image = Image.open(os.path.join(INPUT_DIR, file_name))\n",
    "                image = preprocessor(images=image, return_tensors=\"pt\")\n",
    "            except:\n",
    "                continue\n",
    "            images.append(image['pixel_values'])\n",
    "            names.append(file_name)\n",
    "\n",
    "        images = torch.concat(images, dim=0)\n",
    "    \n",
    "        image_features = clip_model.get_image_features(pixel_values=images.to(clip_model.device))\n",
    "        image_features = image_features.detach().cpu().numpy()\n",
    "        \n",
    "        for file_name, image_feature in zip(names, image_features):\n",
    "            clip_path = os.path.join(OUTPUT_DIR, f'{os.path.splitext(file_name)[0]}.npy')\n",
    "            np.save(clip_path, image_feature[None, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823c07b-361a-4198-9ccc-6ca9b509b0f7",
   "metadata": {},
   "source": [
    "# from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8023c238-7b57-4272-81fd-a046f85ed6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ZIP_PATH = './generated-1119.zip'\n",
    "EMB_DIR = './generated/1119/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a6de8b8-7d08-4513-840a-c21bb9a1bfec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(EMB_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca5964-f967-4bac-9165-64fb9a204baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270b8dd-0f97-44ee-8dc3-f322c0509c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = list()\n",
    "\n",
    "files = set(f.namelist())\n",
    "\n",
    "for file_path in f.namelist():\n",
    "    \n",
    "    if file_path.startswith('generated/image/') and file_path.endswith('.jpg'):\n",
    "        \n",
    "        embedding_path = file_path.replace('/image/', '/embedding/').replace('.jpg', '.npz')\n",
    "        clip_path = file_path.replace('/image/', '/clip/').replace('.jpg', '.npy')\n",
    "    \n",
    "        if embedding_path not in files or clip_path not in files:\n",
    "            continue\n",
    "        \n",
    "        file_paths.append(file_path)\n",
    "    \n",
    "file_names = np.array([os.path.split(i)[-1] for i in file_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f9ad8-2140-4014-966b-228061400802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_embs = list()\n",
    "negative_embs = list()\n",
    "image_embs = list()\n",
    "\n",
    "for file_name in tqdm(file_names):\n",
    "    \n",
    "    embedding_path = os.path.join('generated', 'embedding', file_name.replace('.jpg', '.npz'))\n",
    "    \n",
    "    npz = np.load(f.open(embedding_path))\n",
    "    positive_embs.append(npz['positive_pooler_output'])\n",
    "    negative_embs.append(npz['negative_pooler_output'])\n",
    "    \n",
    "    clip_path = os.path.join('generated', 'clip', file_name.replace('.jpg', '.npy'))\n",
    "    \n",
    "    image_embs.append(np.load(f.open(clip_path)))\n",
    "    \n",
    "positive_embs = np.concatenate(positive_embs, axis=0)\n",
    "negative_embs = np.concatenate(negative_embs, axis=0)\n",
    "image_embs = np.concatenate(image_embs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01c90897-fcfb-471d-bbb3-cb92d5085a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    os.path.join(EMB_DIR, 'clip_vision_emb.npz'), \n",
    "    image_embeds=image_embs\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(EMB_DIR, 'clip_text_emb.npz'), \n",
    "    positive_pooler_outputs=positive_embs,\n",
    "    negative_pooler_outputs=negative_embs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3dfa8-fd5f-438e-a796-1d1bf9166022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
