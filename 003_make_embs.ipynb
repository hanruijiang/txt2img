{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c10062-4aaf-43c5-9a56-0fcc54568679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b124cd8e-1061-4fb8-a2cd-91638a914bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03dc12a2-26ae-408f-91cd-e701c69f7cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 77\n",
    "# MODEL_NAME = 'BAAI/bge-base-en-v1.5'\n",
    "MODEL_NAME = 'openai/clip-vit-large-patch14'\n",
    "# MODEL_NAME = 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'\n",
    "# MODEL_NAME = 'johngiorgi/declutr-base'\n",
    "# MAX_LENGTH = 88\n",
    "# MODEL_NAME = '../Llama-2-7b-hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1744b-72dc-427d-8256-a5e2499ea129",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ca2e84-26f7-4d4d-bbe7-6adc0df0f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../kcg-ml-image-pipeline/output/dataset/data/environmental/'\n",
    "PMT_PATH = 'data/environmental/prompt.json'\n",
    "EMB_PATH = 'data/environmental/clip_text_emb.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb74eb-e538-42f7-9808-92a8cccfcb3a",
   "metadata": {},
   "source": [
    "## save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a18304-cc24-450b-9c08-92ef4dca5cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-vae-test/'))\n",
    "from utilities.utils import read_embedding_data, read_msg_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d73d781-ae7d-495b-8e33-21e121330aef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c4b50919f94067983cb7e86982a94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = sorted(glob.glob(os.path.join(INPUT_DIR, '**/*_data.msgpack')))\n",
    "\n",
    "file_paths = list()\n",
    "file_hashs = list()\n",
    "positive_prompts = list()\n",
    "negative_prompts = list()\n",
    "creation_times = list()\n",
    "for path in tqdm(paths):\n",
    "    mp = read_msg_pack(path)\n",
    "    file_paths.append(mp['file_path'])\n",
    "    file_hashs.append(mp['file_hash'])\n",
    "    positive_prompts.append(mp['positive_prompt'])\n",
    "    negative_prompts.append(mp['negative_prompt'])\n",
    "    creation_times.append(mp['creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1439c9ae-568d-4bdc-854b-4d066bfcf0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(PMT_PATH)[0], exist_ok=True)\n",
    "\n",
    "json.dump(\n",
    "    pd.DataFrame(zip(positive_prompts, negative_prompts, file_paths, creation_times), columns=['positive_prompt', 'negative_prompt', 'file_path', 'creation_time'], index=file_hashs).to_dict(orient='index'),\n",
    "    open(PMT_PATH, 'w')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71358c40-3a87-451d-9d0b-bee09876493a",
   "metadata": {},
   "source": [
    "## load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c6ed04-ec55-47da-b8fd-d044a044c2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_hashs = list()\n",
    "file_paths = list()\n",
    "positive_prompts = list()\n",
    "negative_prompts = list()\n",
    "\n",
    "for file_hash, info in json.load(open(PMT_PATH)).items():\n",
    "    \n",
    "    file_hashs.append(file_hash)\n",
    "    file_paths.append(info['file_path'])\n",
    "    positive_prompts.append(info['positive_prompt'])\n",
    "    negative_prompts.append(info['negative_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5511b-ddfd-4dc9-92b9-ee9b78d8a193",
   "metadata": {},
   "source": [
    "# load text embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8a0b7-0d43-4032-aa65-63ce99cd2792",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## from kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57186428-fa0c-4312-bd9d-68247824e5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ln -s ../kcg-ml-sd1p4/input/ input\n",
    "# !ln -s ../kcg-ml-sd1p4/output/ output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf1b0a0-0642-4ef0-b37c-b6021ff9a067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mINFO: Created a temporary directory at /tmp/tmpejv92m2d\u001b[0m\n",
      "\u001b[1;32mINFO: Writing /tmp/tmpejv92m2d/_remote_module_non_scriptable.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-sd1p4/'))\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7777febf-a149-4c98-9bc7-69f1c2f17008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and transformer\u001b[32m...[DONE]\u001b[0m\u001b[34m\t2,403.54ms\u001b[0m                             \n"
     ]
    }
   ],
   "source": [
    "clip_text_embedder = CLIPTextEmbedder(device='cuda')\n",
    "_ = clip_text_embedder.load_submodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6317b86-eeb7-4f93-bec6-de9394f2b903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts):\n",
    "    \n",
    "    clip_text_opt = clip_text_embedder.forward_return_all(texts)\n",
    "\n",
    "    last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    attention_mask = clip_text_opt.attention_mask.detach().cpu().numpy()\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79377829-7d62-4df6-9f54-61f12ea33d78",
   "metadata": {},
   "source": [
    "## from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019884b2-42d8-4c74-b41a-9f725b10c7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e2f96-d4c9-4ac0-bca0-f4f75cd98b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### from CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559844df-0456-4fab-87ff-04f59a2749f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).text_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0490001b-a764-4abb-970e-a8c0d1ede92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts, use_penultimate=False):\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "    clip_text_opt = transformer(input_ids=tokens, output_hidden_states=True, return_dict=True)\n",
    "    \n",
    "    attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    \n",
    "    if use_penultimate:\n",
    "        last_hidden_state = clip_text_opt.hidden_states[-1].detach().cpu().numpy()\n",
    "    else:\n",
    "        last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580208c-b733-47bd-af51-9841bea563b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14127512-6084-4737-ac7d-4af8ae09517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts):\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "    clip_text_opt = transformer(input_ids=tokens)\n",
    "\n",
    "    last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    \n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    # pooler_output = None\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d8a83a-eda4-46e2-8d0a-7e29194f3d69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /johngiorgi/declutr-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fdb2e5ce7d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 844b863d-ff1b-4646-a032-384b3cb8a040)')' thrown while requesting HEAD https://huggingface.co/johngiorgi/declutr-base/resolve/main/config.json\n",
      "Some weights of the model checkpoint at johngiorgi/declutr-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(MODEL_NAME).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbadf41-a3c2-4bb2-aaa8-f0e8d5c99e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14a1ff-fb5b-42de-8143-447b1761a3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dccc6dafc5a4dcda5c7b144d0d42744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../Llama-2-7b-hf were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "transformer = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, load_in_8bit=True, device_map='auto').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360503-30e0-48d3-b0de-6f3d3ab4b63b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# embed & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52457393-ceb4-46fc-9ef3-c143cc7faf67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b49640eab424aeba7144738d22a3b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59f9a409a944d61b12d1ed60cb810c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_last_hidden_states = list()\n",
    "positive_pooler_outputs = list()\n",
    "positive_attention_masks = list()\n",
    "\n",
    "negative_last_hidden_states = list()\n",
    "negative_pooler_outputs = list()\n",
    "negative_attention_masks = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(0, len(positive_prompts), BATCH_SIZE)):\n",
    "        \n",
    "        last_hidden_state, pooler_output, attention_mask = worker(positive_prompts[i:i+BATCH_SIZE])\n",
    "        \n",
    "        # positive_last_hidden_states.append(last_hidden_state)\n",
    "        # positive_attention_masks.append(attention_mask)\n",
    "        if pooler_output is not None:\n",
    "            positive_pooler_outputs.append(pooler_output)\n",
    "    \n",
    "    for i in tqdm(range(0, len(negative_prompts), BATCH_SIZE)):\n",
    "        \n",
    "        last_hidden_state, pooler_output, attention_mask = worker(negative_prompts[i:i+BATCH_SIZE])\n",
    "        \n",
    "#         negative_last_hidden_states.append(last_hidden_state)\n",
    "#         negative_attention_masks.append(attention_mask)\n",
    "        if pooler_output is not None:\n",
    "            negative_pooler_outputs.append(pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c188adf-636d-48f5-b615-191ec1aa7e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positive_last_hidden_states = np.concatenate(positive_last_hidden_states, axis=0)\n",
    "# positive_attention_masks = np.concatenate(positive_attention_masks, axis=0)\n",
    "if len(positive_pooler_outputs) > 0:\n",
    "    positive_pooler_outputs = np.concatenate(positive_pooler_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d0a4fb0-cf53-46bd-be81-534ce0aaadc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# negative_last_hidden_states = np.concatenate(negative_last_hidden_states, axis=0)\n",
    "# negative_attention_masks = np.concatenate(negative_attention_masks, axis=0)\n",
    "if len(positive_pooler_outputs) > 0:\n",
    "    negative_pooler_outputs = np.concatenate(negative_pooler_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa5afb5c-35ee-40a1-b6e8-e0046efb3738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(EMB_PATH)[0], exist_ok=True)\n",
    "\n",
    "np.savez(\n",
    "    EMB_PATH, \n",
    "    file_hashs=np.array(file_hashs), \n",
    "    file_paths=np.array(file_paths), \n",
    "    # positive_last_hidden_states=positive_last_hidden_states, \n",
    "    # positive_attention_masks=positive_attention_masks,\n",
    "    positive_pooler_outputs=positive_pooler_outputs,\n",
    "    # negative_last_hidden_states=negative_last_hidden_states,\n",
    "    # negative_attention_masks=negative_attention_masks,\n",
    "    negative_pooler_outputs=negative_pooler_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd5a93-65ff-4fb9-a8ff-b4f5200de3be",
   "metadata": {},
   "source": [
    "# vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ca42ef-3392-4963-a43d-73369118ca2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = CLIPImageProcessor.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8f46c-ac85-4c02-8a89-1d7790a70eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced115f8-2ba4-48bb-bb24-70425cf6ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = list()\n",
    "\n",
    "for file_name in os.listdir(os.path.join(OUTPUT_DIR, 'image')):\n",
    "    clip_path = os.path.join(OUTPUT_DIR, 'clip', file_name.replace('.jpg', '.npy'))\n",
    "    \n",
    "    if os.path.exists(clip_path):\n",
    "        continue\n",
    "        \n",
    "    file_names.append(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
