{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484c6fa5-fef0-4582-8ff2-8d47917f01e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f74571f-ff5d-480d-a622-5043962533f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1b002e-bf95-4aea-8dd2-ed8ae880b7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimizers import Adan, Lookahead, AGC\n",
    "from utils import get_score_from_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f1c8ea-90b6-4969-a065-29b5f1500ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = '../kcg-ml-image-pipeline/output/dataset/'\n",
    "\n",
    "dataset_name = 'environmental'\n",
    "# dataset_name = 'character'\n",
    "# dataset_name = 'mech'\n",
    "# dataset_name = 'icons'\n",
    "# dataset_name = 'waifu'\n",
    "# dataset_name = 'propaganda-poster'\n",
    "\n",
    "# use_positive = True\n",
    "# use_negative = True\n",
    "# pooling_method = 'pooler_outputs'\n",
    "\n",
    "# EMB_PATH = os.path.join('./data', dataset_name, 'clip_text_emb.npz')\n",
    "# WEIGHT_PATH = os.path.join('weight/004', dataset_name, 'clip_text.pt')\n",
    "# WEIGHT_PATH = os.path.join('weight/004', dataset_name, 'clip_positive.pt')\n",
    "# WEIGHT_PATH = os.path.join('weight/004', dataset_name, 'clip_negative.pt')\n",
    "\n",
    "use_positive = True\n",
    "use_negative = False\n",
    "pooling_method = 'image_embeds'\n",
    "\n",
    "EMB_PATH = os.path.join('./data', dataset_name, 'clip_vision_emb.npz')\n",
    "WEIGHT_PATH = os.path.join('weight/004', dataset_name, 'clip_vision.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc34cd1-3e8f-49aa-ab0b-48272dab7c63",
   "metadata": {},
   "source": [
    "# load emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0588ce8-1cad-4f43-b320-7035a814aa0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load(EMB_PATH, allow_pickle=True)\n",
    "\n",
    "file_paths = data['file_paths']\n",
    "file_paths = [os.path.splitext(file_path.split('_')[0])[0] for file_path in file_paths]\n",
    "path_to_index = {file_path: i for i, file_path in enumerate(file_paths)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5773745-3567-46e1-8d04-83c58ce19971",
   "metadata": {},
   "source": [
    "# load rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0d23bb-b4ec-4968-876a-b9b953924604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e932eb8d8175402c8b5acaca272b9021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = sorted(glob.glob(os.path.join(ROOT, 'ranking', dataset_name, '*.json')))\n",
    "\n",
    "rank_pairs = list()\n",
    "for path in tqdm(paths):\n",
    "    js = json.load(open(path))\n",
    "    \n",
    "    file_path_1 = os.path.splitext(js['image_1_metadata']['file_path'])[0].replace('datasets/', '')\n",
    "    file_path_2 = os.path.splitext(js['image_2_metadata']['file_path'])[0].replace('datasets/', '')\n",
    "    \n",
    "    if (file_path_1 not in path_to_index) or (file_path_2 not in path_to_index):\n",
    "        continue\n",
    "    rank_pairs.append((file_path_1, file_path_2, js['selected_image_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64249b-1f2e-4b49-8276-b212a025865a",
   "metadata": {},
   "source": [
    "# build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acacb29d-b8eb-48db-ae01-0293e4010e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_pairs = pd.DataFrame(rank_pairs, columns=['image_1', 'image_2', 'selected_image_index'])\n",
    "\n",
    "ordered_pairs = [((image_1, image_2) if selected_image_index == 0 else (image_2, image_1)) for image_1, image_2, selected_image_index in rank_pairs.itertuples(index=False, name=None)]\n",
    "ordered_pairs = pd.DataFrame(ordered_pairs, columns=['image_1', 'image_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0bf066c-a6ea-4345-8b14-f828a34d83af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(ordered_pairs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c49800b0-3591-471c-b444-cd348d81ed6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if pooling_method == 'pooler_outputs':\n",
    "    positive_pooler_outputs = data['positive_pooler_outputs']\n",
    "    negative_pooler_outputs = data['negative_pooler_outputs']\n",
    "elif pooling_method == 'image_embeds':\n",
    "    image_embeds = data['image_embeds'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744e1e6e-98a6-4a07-84a6-8b35a7e65722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positive_last_hidden_states = data['positive_last_hidden_states']\n",
    "# positive_attention_masks = data['positive_attention_masks']\n",
    "# negative_last_hidden_states = data['negative_last_hidden_states']\n",
    "# negative_attention_masks = data['negative_attention_masks']\n",
    "\n",
    "# positive_last_hidden_states_ave_mean = positive_last_hidden_states.mean(axis=1)\n",
    "# negative_last_hidden_states_ave_mean = negative_last_hidden_states.mean(axis=1)\n",
    "\n",
    "# positive_last_hidden_states_ave_mean_with_mask = (positive_last_hidden_states * positive_attention_masks[..., None]).sum(axis=1) / positive_attention_masks.sum(axis=-1)[..., None]\n",
    "# negative_last_hidden_states_ave_mean_with_mask = (negative_last_hidden_states * negative_attention_masks[..., None]).sum(axis=1) / negative_attention_masks.sum(axis=-1)[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eb257-225c-4222-9070-21b5512d5cd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## build feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0132fe-c342-4b1a-9eb4-94af3f2c3d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature(index_1, index_2):\n",
    "    \n",
    "    if pooling_method == 'pooler_outputs':\n",
    "        pos_features = positive_pooler_outputs\n",
    "        neg_features = negative_pooler_outputs\n",
    "    elif pooling_method == 'ave_mean':\n",
    "        pos_features = positive_last_hidden_states_ave_mean\n",
    "        neg_features = negative_last_hidden_states_ave_mean\n",
    "    # elif pooling_method == 'ave_mean_with_mask':\n",
    "    #     pos_features = positive_last_hidden_states_ave_mean_with_mask\n",
    "    #     neg_features = negative_last_hidden_states_ave_mean_with_mask\n",
    "    elif pooling_method == 'image_embeds':\n",
    "        pos_features = image_embeds\n",
    "    \n",
    "    results = list()\n",
    "    if use_positive:\n",
    "        results.append(np.stack([pos_features[index_1], pos_features[index_2]], axis=-1))\n",
    "    if use_negative:\n",
    "        results.append(np.stack([neg_features[index_1], neg_features[index_2]], axis=-1))\n",
    "        \n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return np.concatenate(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4224f943-eeb1-47e3-97ab-1abc6671d8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56863, 768, 2), (14216, 768, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list()\n",
    "for image_1, image_2 in train_pairs.itertuples(index=False, name=None):\n",
    "    index_1, index_2 = path_to_index[image_1], path_to_index[image_2]\n",
    "    train_data.append(build_feature(index_1, index_2))\n",
    "train_data = np.stack(train_data, axis=0)\n",
    "\n",
    "val_data = list()\n",
    "for image_1, image_2 in val_pairs.itertuples(index=False, name=None):\n",
    "    index_1, index_2 = path_to_index[image_1], path_to_index[image_2]\n",
    "    val_data.append(build_feature(index_1, index_2))\n",
    "val_data = np.stack(val_data, axis=0)\n",
    "\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64b9f52-3fde-4297-bc91-23e6f71b5165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.tensor(train_data).half().cuda()\n",
    "val_dataset = torch.tensor(val_data).half().cuda()\n",
    "# ext_dataset = torch.tensor(ext_data).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a81a3-7fea-45dd-b631-b613dfd24348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e736cc1a-ab3a-4b97-986b-beb6a41fab29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = torch.nn.Linear(train_data.shape[1], 1, bias=True)\n",
    "model = torch.nn.Linear(train_data.shape[1], 1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95ea89c-513d-4506-b933-3cf9344594d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(WEIGHT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af1b92d-e273-48b1-85dc-1ec8b5bc14b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = torch.nn.DataParallel(model.cuda())\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790b38f-8a6a-4606-9cc4-0ef9719920a5",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7155774-b0e0-41ed-b77f-4a03a31e28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "\n",
    "optimizer = Adan(parameters, lr=LR, weight_decay=1e-3)\n",
    "optimizer = Lookahead(optimizer)\n",
    "# optimizer = AGC(optimizer)\n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, [lambda step: step / 100. if step < 100 else 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782ee7eb-8c01-48ba-b6c5-e3367299e3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beef869592974c6ebba4d14b030ae692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhxie/snap/kk-digital/kcg-ml-image-pipeline/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4983 0.7401 0.2949 0.8734\n",
      "0.2451 0.8962 0.2277 0.9048\n",
      "0.2172 0.9087 0.2150 0.9091\n",
      "0.2075 0.9144 0.2091 0.9144\n",
      "0.2030 0.9167 0.2056 0.9169\n",
      "0.1998 0.9181 0.2035 0.9177\n",
      "0.1976 0.9191 0.2020 0.9181\n",
      "0.1963 0.9201 0.2010 0.9183\n",
      "0.1952 0.9203 0.2006 0.9184\n",
      "0.1947 0.9205 0.2006 0.9177\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "bces, accs = list(), list()\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    label = torch.zeros((train_dataset.shape[0],), device='cuda')\n",
    "    \n",
    "    x = train_dataset\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.cuda.amp.autocast(True):\n",
    "\n",
    "        y0 = model(x[..., 0])\n",
    "        y1 = model(x[..., 1])\n",
    "\n",
    "        y = torch.concat([y0, y1], dim=-1)\n",
    "\n",
    "    # backward\n",
    "\n",
    "    bce = torch.nn.functional.cross_entropy(y, label.long())\n",
    "\n",
    "    acc = (y0 > y1).float().mean()\n",
    "\n",
    "    l1 = torch.norm(model.weight, p=1)\n",
    "\n",
    "    loss = bce + l1 * 1e-3\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    warmup.step()\n",
    "\n",
    "    bces.append(bce.detach().cpu().numpy())\n",
    "    accs.append(acc.detach().cpu().numpy())\n",
    "        \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        val_bces, val_accs = list(), list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x = val_dataset\n",
    "\n",
    "            with torch.cuda.amp.autocast(True):\n",
    "\n",
    "                y0 = model(x[..., 0])\n",
    "                y1 = model(x[..., 1])\n",
    "\n",
    "                y = torch.concat([y0, y1], dim=-1)\n",
    "\n",
    "            label = torch.zeros((y.shape[0],), device='cuda').long()\n",
    "\n",
    "            bce = torch.nn.functional.cross_entropy(y, label)\n",
    "\n",
    "            acc = (y.argmax(dim=-1) == 0).float().mean()\n",
    "\n",
    "            val_bces.append(bce.detach().cpu().numpy())\n",
    "            val_accs.append(acc.detach().cpu().numpy())\n",
    "\n",
    "        print(f'{np.mean(bces):.4f} {np.mean(accs):.4f} {np.mean(val_bces):.4f} {np.mean(val_accs):.4f}')\n",
    "    \n",
    "        bces, accs = list(), list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935c876-24ae-467f-8cb7-e482a0270357",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67aa1c79-3dd6-4f2a-a5c6-e19084136f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(WEIGHT_PATH)[0], exist_ok=True)\n",
    "torch.save(model.state_dict(), WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d15c3e-8772-45fd-9c2a-b77a8fd6be6f",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962232b2-e91e-4569-bb5e-76a014b22867",
   "metadata": {},
   "source": [
    "## score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29747560-e33e-4c1f-88e3-b30ca09720e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = get_score_from_embs(image_embeds, model, batch_size=1024)\n",
    "# score = get_score_from_embs(np.concatenate([positive_pooler_outputs, negative_pooler_outputs], axis=-1), model, batch_size=1024)\n",
    "# score = get_score_from_embs(positive_pooler_outputs, model, batch_size=1024)\n",
    "# score = get_score_from_embs(negative_pooler_outputs, model, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77d95600-375b-49f5-b0ba-fe56980c9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump({\n",
    "    'mean': float(score.mean()),\n",
    "    'std': float(score.std()),\n",
    "}, open(WEIGHT_PATH.replace('.pt', '_stats.json'), 'wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5f039-1da9-4ba2-a782-80d628e6c94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = pyplot.hist(score, bins=100, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ceb83-c935-402d-8152-eeca817814db",
   "metadata": {},
   "source": [
    "## val set distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab1386-eea9-4cda-9c8d-4ebc2a395112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y0 = get_score_from_embs(val_dataset[..., 0].float(), model, 1024)\n",
    "y1 = get_score_from_embs(val_dataset[..., 1].float(), model, 1024)\n",
    "\n",
    "delta = y0 - y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd84a89-17ee-489f-b85e-b1dddfddd672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10, 4))\n",
    "\n",
    "pyplot.subplot(1, 2, 1)\n",
    "\n",
    "correct = (y0 > y1)\n",
    "\n",
    "_ = pyplot.hist(y0[correct], bins=100, color='r', alpha=0.5)\n",
    "_ = pyplot.hist(y1[correct], bins=100, color='b', alpha=0.5)\n",
    "\n",
    "pyplot.title('score distribution of correct pairs')\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "\n",
    "_ = pyplot.hist(y0[~correct], bins=100, color='r', alpha=0.5)\n",
    "_ = pyplot.hist(y1[~correct], bins=100, color='b', alpha=0.5)\n",
    "\n",
    "pyplot.title('score distribution of incorrect pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35690-baac-43ad-b807-43c4d16eb311",
   "metadata": {},
   "source": [
    "## prepare for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d57b7-9731-4c64-9faa-5bfa3c7c2c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_samples(indices, n_select):\n",
    "    \n",
    "    selected = np.random.choice(indices, n_select, False)\n",
    "    \n",
    "    selected_file_paths = [file_paths[i] for i in selected]\n",
    "    selected_file_paths = [i.split('_')[0] + '.jpg' for i in selected_file_paths]\n",
    "    \n",
    "    images = np.stack([np.array(Image.open(os.path.join('../kcg-ml-image-pipeline/output/dataset/image/', i))) for i in selected_file_paths])\n",
    "    images = images.reshape(-1, int(n_select ** 0.5), *images.shape[-3:])\n",
    "    images = np.concatenate(np.concatenate(images, axis=-3), axis=-2)\n",
    "    \n",
    "    images = Image.fromarray(images).resize((512, 512))\n",
    "    \n",
    "    return selected, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e9629-1de5-458c-b128-c6d7dc2f34ee",
   "metadata": {},
   "source": [
    "## show top score samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439198a3-782b-4673-89cf-3b70a7247fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = np.quantile(score, q=0.95)\n",
    "selected, images = select_samples(np.arange(score.shape[0])[score > threshold], n_select=9)\n",
    "\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810172c-5638-4320-a700-44e3e630095d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d7c03-43f9-4aca-802e-4a63cf08f05f",
   "metadata": {},
   "source": [
    "## show lowest score samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb389e4-a197-499f-a00a-6a7ddac70750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = np.quantile(score, q=0.05)\n",
    "selected, images = select_samples(np.arange(score.shape[0])[score < threshold], n_select=9)\n",
    "\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112a95a-0eb0-4c84-ac98-779fde2f6cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be567dbf-ca04-4927-aa51-79cb0bd1f147",
   "metadata": {},
   "source": [
    "## show lowest delta samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6fe7a-ba52-48a9-a1ab-06f59bc2600e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_select = 6\n",
    "\n",
    "# threshold = np.quantile(val_delta, q=0.05)\n",
    "# indices = val_pairs.index[np.arange(val_delta.shape[0])[val_delta < threshold]]\n",
    "# selected = np.random.choice(indices, n_select, False)\n",
    "\n",
    "selected = val_pairs.index[np.argsort(delta)[:n_select]]\n",
    "\n",
    "indices_1 = [path_to_index[i] for i in val_pairs.loc[selected, 'image_1']]\n",
    "indices_2 = [path_to_index[i] for i in val_pairs.loc[selected, 'image_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673b385-bb54-4468-b077-08093f6daf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_file_paths = [file_paths[i] for i in indices_1]\n",
    "selected_file_paths = [i.split('_')[0] + '.jpg' for i in selected_file_paths]\n",
    "\n",
    "images = np.stack([np.array(Image.open(os.path.join('../kcg-ml-image-pipeline/output/dataset/image/', i))) for i in selected_file_paths])\n",
    "images_1 = np.concatenate(images, axis=-2)\n",
    "\n",
    "selected_file_paths = [file_paths[i] for i in indices_2]\n",
    "selected_file_paths = [i.split('_')[0] + '.jpg' for i in selected_file_paths]\n",
    "\n",
    "images = np.stack([np.array(Image.open(os.path.join('../kcg-ml-image-pipeline/output/dataset/image/', i))) for i in selected_file_paths])\n",
    "images_2 = np.concatenate(images, axis=-2)\n",
    "\n",
    "images = np.concatenate([images_1, images_2], axis=-3)\n",
    "\n",
    "images = Image.fromarray(images).resize((512 * n_select // 2, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817f9df-c7af-4fd1-af2c-5128beb6395e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fc9b0-1caa-4d65-955f-fdc78dca8aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852d0257-87ac-40e4-b246-b9eec420c79c",
   "metadata": {},
   "source": [
    "# check conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be13a49-2a7b-4272-8fb7-e81f684d8c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdb763-ee57-4137-aad3-265d3af4744b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = networkx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233d2e4-774c-4ebb-8eb0-f17733a9b1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_1, img_2, sel_id in rank_pairs:\n",
    "    if sel_id == 0:\n",
    "        graph.add_edge(img_2, img_1)\n",
    "    else:\n",
    "        graph.add_edge(img_1, img_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3d971-c385-4f26-b660-796dafa31fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(graph.nodes), len(graph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65815de0-5895-46b1-a45b-42468270df42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycles = list(networkx.simple_cycles(graph))\n",
    "len(cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f22964-9918-40d0-8a6f-1666d2b281cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subgraphs = list(networkx.weakly_connected_components(graph))\n",
    "len(subgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2d688-1612-488e-a590-0db4ce7a9e92",
   "metadata": {},
   "source": [
    "# check transitive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3cf9c-bbc6-491e-b4bc-75f0ef2c31ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_pairs = list()\n",
    "\n",
    "for image_2, d in networkx.all_pairs_shortest_path_length(graph):\n",
    "    \n",
    "    for image_1, dist in d.items():\n",
    "        \n",
    "        if dist <= 1:\n",
    "            continue\n",
    "        \n",
    "        trans_pairs.append((image_1, image_2, dist))\n",
    "        \n",
    "trans_pairs = pd.DataFrame(trans_pairs, columns=['image_1', 'image_2', 'dist'])\n",
    "trans_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cbb2f-869c-4e86-8aeb-1e490b43c049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
